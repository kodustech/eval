import * as dotenv from 'dotenv';
import { Evaluator } from './Evaluator';
import { EvaluatorConfig } from './types';

// Carrega variÃ¡veis de ambiente
dotenv.config();

async function testSingleCase() {
  try {
    console.log('ğŸ§ª Testando evaluator com um caso simples...\n');

    // ConfiguraÃ§Ã£o bÃ¡sica
    const config: EvaluatorConfig = {
      reviewerLLM: {
        provider: 'openai',
        model: 'gpt-4o-mini',
        apiKey: process.env.OPENAI_API_KEY || '',
        temperature: 0.1,
        maxTokens: 2000,
      },
      evaluatorLLM: {
        provider: 'openai',
        model: 'gpt-4o-mini',
        apiKey: process.env.OPENAI_API_KEY || '',
        temperature: 0.1,
        maxTokens: 2000,
      },
      testCases: []
    };

    const evaluator = new Evaluator(config);
    
    // Lista os test cases disponÃ­veis
    const available = evaluator.getAvailableTestCases();
    console.log('ğŸ“ Test cases disponÃ­veis:', available);
    
    if (available.length === 0) {
      console.log('âŒ Nenhum test case encontrado na pasta examples/');
      return;
    }

    // Testa o primeiro disponÃ­vel
    const testCase = available[0];
    console.log(`\nğŸ¯ Testando: ${testCase}`);
    
    const results = await evaluator.evaluateAllTestCases([testCase]);
    
    if (results.length > 0) {
      const result = results[0];
      console.log('\nâœ… Teste concluÃ­do!');
      console.log(`ğŸ“Š Accuracy: ${result.accuracy.toFixed(1)}%`);
      console.log(`ğŸ¯ Matches: ${result.matchedSuggestions}/${result.totalSuggestions}`);
      console.log(`ğŸ“ AnÃ¡lise: ${result.detailedAnalysis.substring(0, 200)}...`);
    }
    
  } catch (error) {
    console.error('âŒ Erro no teste:', error);
    
    if (error instanceof Error && error.message.includes('API key')) {
      console.log('\nğŸ’¡ Dica: Configure sua OPENAI_API_KEY no arquivo .env');
    }
  }
}

testSingleCase(); 